{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9abe0e99-3204-4a42-ac4c-48c9e460b15a",
   "metadata": {},
   "source": [
    "# Why Preprocess?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eccaf8-51d8-476a-98f9-95838c1d145d",
   "metadata": {},
   "source": [
    "Preprocessing data is required for making the machine learning model work efficiently and get correct and best results, all the raw data are mostly not ready to be computed, so we clean and prepare the data, this process is considered as Data Preprocessing. Preprocessing can involve things like scaling numbers to be within a certain range, normalizing the data to avoid huge differences between values, or turning words into numbers that computers can understand.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc352197-158e-401a-b844-272a7084588a",
   "metadata": {},
   "source": [
    "### PreProccesing Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0eafd2-f8c5-453e-b3cd-c2e7f1207447",
   "metadata": {},
   "source": [
    "### `Standardization or Mean Removal`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd770556-c866-401d-84a8-7610aec02f46",
   "metadata": {},
   "source": [
    "### Dataset Example: Standardization\n",
    "\n",
    "Let’s say we have a small dataset of test scores for five students:\n",
    "\n",
    "$70, 80, 90, 100, 110$\n",
    "\n",
    "### Step 1: Find the mean (average)  \n",
    "To calculate the mean:  \n",
    "\n",
    "$\\text{mean} = \\frac{70 + 80 + 90 + 100 + 110}{5} = \\frac{450}{5} = 90$\n",
    "\n",
    "---\n",
    "### Step 2: Subtract the mean from each value  \n",
    "Now, we subtract 90 (the mean) from each score:\n",
    "\n",
    "$70 - 90 = -20$  \n",
    "$80 - 90 = -10$  \n",
    "$90 - 90 = 0$  \n",
    "$100 - 90 = 10$  \n",
    "$110 - 90 = 20$\n",
    "\n",
    "So the adjusted scores are:  \n",
    "\n",
    "$-20, -10, 0, 10, 20$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Divide by the standard deviation  \n",
    "Next, we calculate the standard deviation. For simplicity, let’s assume the standard deviation is 15 (in reality, you would calculate it, but we'll use this number for this example).\n",
    "\n",
    "Now we divide each adjusted score by 15 (the standard deviation):\n",
    "\n",
    "$\\frac{-20}{15} \\approx -1.33$  \n",
    "$\\frac{-10}{15} \\approx -0.67$  \n",
    "$\\frac{0}{15} = 0$  \n",
    "$\\frac{10}{15} \\approx 0.67$  \n",
    "$\\frac{20}{15} \\approx 1.33$\n",
    "\n",
    "So, the standardized scores are approximately:  \n",
    "\n",
    "$-1.33, -0.67, 0, 0.67, 1.33$\n",
    "\n",
    "These values are now centered around 0, and have no bias towards larger or smaller numbers, making it easier for machine learning algorithms to process the data.\n",
    "\n",
    "---\n",
    "\n",
    "When we say the values are \"centered around 0\" and \"have no bias towards larger or smaller numbers,\" we mean that, after standardization, the data is more balanced and easier for machine learning algorithms to handle. Here’s why:\n",
    "\n",
    "### Centered around 0:\n",
    "After subtracting the mean, the average of all the values becomes 0. Some values will be slightly below zero (negative), and some will be above zero (positive). This makes the data more evenly distributed around a central point, helping algorithms not to focus too much on larger numbers.\n",
    "\n",
    "### No bias towards larger or smaller numbers:\n",
    "Before standardization, one feature (like a test score of 110) might have been much larger than another (like a score of 70). This could cause a machine learning model to give more importance to the bigger numbers, even though that might not be helpful. Standardization adjusts the scale of the values so that they’re all in a similar range, and no value dominates just because it’s larger.\n",
    "\n",
    "In short, standardization ensures that all features are treated equally by the model, without any one feature skewing the results due to its size. This makes it easier for algorithms to learn patterns from the data fairly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1207151c-4766-457b-a153-e0053d5410ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e418fc1c-52ba-4b14-aeb9-3a217144200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[8,-3,4,-7],[4,-3,6,8],[6,6,-2,1],[5,6,7,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a802e880-03e4-4e05-a992-f66b3eaa5a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  [5.75 1.5  3.75 2.5 ]\n",
      "Standard Deviation:  [1.47901995 4.5        3.49106001 6.18465844]\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean: \",data.mean(axis=0))\n",
    "print('Standard Deviation: ',data.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "554f519e-1378-4a4e-bff9-d10301ac76cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_standardized= preprocessing.scale(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3561ceaa-1725-4d08-8a75-e2d428511bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean standardized data:  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -2.77555756e-17]\n",
      "Standard Deviation standardized data:  [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean standardized data: \",data_standardized.mean(axis=0))\n",
    "print(\"Standard Deviation standardized data: \",data_standardized.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1cf06b0f-2fec-44d0-ab6c-bec02e3a9eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean standardized data:  [ 0.  0.  0. -0.]\n",
      "Standard Deviation standardized data:  [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean standardized data: \", np.round(data_standardized.mean(axis=0), 2))\n",
    "print(\"Standard Deviation standardized data: \", data_standardized.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a21e3249-8b34-4eea-8133-2119ef905725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.52127766 -1.          0.07161149 -1.53605896]\n",
      " [-1.18321596 -1.          0.64450339  0.88929729]\n",
      " [ 0.16903085  1.         -1.64706421 -0.24253563]\n",
      " [-0.50709255  1.          0.93094934  0.88929729]]\n"
     ]
    }
   ],
   "source": [
    "print(data_standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd201a1-5c89-40eb-9693-d0df4a30b70b",
   "metadata": {},
   "source": [
    "### `Z-Score Standardization`\n",
    "---\n",
    "The `sklearn.preprocessing` package includes helpful tools for adjusting the features of your data to make them easier to work with. One common method is the `scale()` function, which performs z-score standardization. \n",
    "\n",
    "- **What is a z-score?**  \n",
    "  A z-score tells you how far away a particular data point is from the average (mean) of the dataset. It shows this distance in terms of standard deviations, which are a measure of how spread out the data is.\n",
    "  \n",
    "  - **Positive z-scores** mean that the data point is above the average.\n",
    "  - **Negative z-scores** mean that the data point is below the average.\n",
    "  \n",
    "In simpler terms, z-scores help you understand where a specific value stands compared to the average of the whole set. \n",
    "\n",
    "---\n",
    "### Importance of Standardization\n",
    "\n",
    "Standardization is especially important when we don’t know the minimum and maximum values in our data. In such cases, we can’t use other methods that rely on knowing these boundaries. \n",
    "\n",
    "- **What happens after standardization?**  \n",
    "  After standardization, the new values (z-scores) don’t have specific minimum or maximum limits. They’re simply adjusted around the average. \n",
    "\n",
    "- **How does it handle outliers?**  \n",
    "  This technique is less affected by extreme values (outliers) compared to other methods. So, even if there are some very high or low values in the data, z-score standardization still works effectively.\n",
    "\n",
    "In summary, standardization with z-scores is a way to make data easier to compare and analyze, especially when we don’t know the limits of the data or when there are outliers present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1271b07-3a73-4b88-8a2e-32d7740d66b0",
   "metadata": {},
   "source": [
    "### `Data Scaling`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d739fe3d-4962-4d65-8017-ea6f848e1356",
   "metadata": {},
   "source": [
    "### Importance of Scaling Data in Machine Learning\n",
    "\n",
    "When working with machine learning algorithms, it is crucial to scale your data before training the model. Scaling, or rescaling, involves adjusting the range of the data features so that they have a common scale without distorting differences in the ranges of values.\n",
    "\n",
    "---\n",
    "#### Why Scale Data?\n",
    "\n",
    "1. **Elimination of Units:**\n",
    "   Scaling removes the units of measurement from the data. This means that all features are treated equally, regardless of their original units (e.g., meters, kilometers, pounds). This is particularly important when features have different units or scales, as it ensures that no single feature dominates the model due to its magnitude.\n",
    "\n",
    "2. **Improved Comparability:**\n",
    "   When data is scaled, it becomes easier to compare features from different locations or sources. For example, if you are comparing temperature data from two cities, scaling allows you to analyze how temperatures relate to each other without the influence of differing measurement systems or ranges.\n",
    "\n",
    "3. **Enhanced Algorithm Performance:**\n",
    "   Many machine learning algorithms, such as gradient descent-based methods (like linear regression and neural networks), converge faster and perform better when the data is scaled. This is because scaled data helps the algorithm to navigate the feature space more efficiently, reducing the chances of getting stuck in local minima.\n",
    "\n",
    "4. **Minimized Sensitivity to Outliers:**\n",
    "   Scaling can help mitigate the impact of outliers, as it brings all features into a similar range. This is especially useful for algorithms that are sensitive to the scale of the input data.\n",
    "\n",
    "---\n",
    "### Common Scaling Techniques\n",
    "\n",
    "- **Min-Max Scaling:** Rescales the data to a fixed range, usually [0, 1].\n",
    "- **Standardization (Z-score normalization):** Centers the data around the mean with a standard deviation of 1.\n",
    "\n",
    "In summary, scaling data is an essential preprocessing step in machine learning that improves the performance and accuracy of models by ensuring that all features are on a comparable scale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b4d248-bcda-4c75-b4a7-01ca714d69e3",
   "metadata": {},
   "source": [
    "### Min-Max Scaler\n",
    "\n",
    "The Min-Max Scaler is a popular technique for scaling features in machine learning. It transforms the data into a specified range, typically [0, 1]. This ensures that all features contribute equally to the model training process.\n",
    "\n",
    "---\n",
    "#### How Min-Max Scaling Works\n",
    "\n",
    "The Min-Max scaling process involves the following steps:\n",
    "\n",
    "1. **Identify Minimum and Maximum Values:**\n",
    "   For each feature (column) in the dataset, identify the minimum value (\\(X_{min}\\)) and the maximum value (\\(X_{max}\\)).\n",
    "\n",
    "2. **Apply the Min-Max Formula:**\n",
    "   Use the following formula to scale each value (\\(X\\)) in the dataset:\n",
    "\n",
    "   $\n",
    "   X' = \\frac{X - X_{min}}{X_{max} - X_{min}}\n",
    "   $\n",
    "\n",
    "   Here, \\(X'\\) is the scaled value, \\(X\\) is the original value, \\(X_{min}\\) is the minimum value of the feature, and \\(X_{max}\\) is the maximum value of the feature.\n",
    "\n",
    "3. **Resulting Range:**\n",
    "   After applying the formula, all the transformed values will lie within the range [0, 1]. This allows for easier comparison of features with different original scales.\n",
    "\n",
    "---\n",
    "#### Advantages of Min-Max Scaling\n",
    "\n",
    "- **Uniform Scale:** It brings all features into the same scale, which can improve the performance of algorithms that are sensitive to the scale of data.\n",
    "- **Preserves Relationships:** Min-Max scaling maintains the relationships between the values, which is beneficial for many machine learning algorithms.\n",
    "\n",
    "#### Disadvantages of Min-Max Scaling\n",
    "\n",
    "- **Sensitive to Outliers:** Since the minimum and maximum values determine the scaling, outliers can significantly affect the scaled values. If an outlier is present, it can compress the range of other values.\n",
    "- **Does Not Center the Data:** Min-Max scaling does not center the data around zero, which may not be ideal for some algorithms that assume normally distributed data.\n",
    "\n",
    "In summary, the Min-Max Scaler is a straightforward and effective way to scale features, making it an essential tool in the data preprocessing phase of machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "08c28a48-ab6f-4995-9e4e-273c41a34b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4dcc1bf6-57de-4d5f-ba62-a2fdbba1c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled = data_scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "30e7612e-0dac-416f-96f3-2d54aeee80b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  [ 4 -3 -2 -7]\n",
      "Max:  [8 6 7 8]\n"
     ]
    }
   ],
   "source": [
    "print(\"Min: \",data.min(axis=0))\n",
    "print(\"Max: \",data.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8ab1a058-717d-4b98-b900-86fe143994e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  [0. 0. 0. 0.]\n",
      "Max:  [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Min: \",data_scaled.min(axis=0))\n",
    "print(\"Max: \",data_scaled.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1d0e6f23-ae25-4bdf-8717-bc7141a0940b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         0.66666667 0.        ]\n",
      " [0.         0.         0.88888889 1.        ]\n",
      " [0.5        1.         0.         0.53333333]\n",
      " [0.25       1.         1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(data_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6363a07a-3f2c-4b2d-85c9-c3bfd35e7dd0",
   "metadata": {},
   "source": [
    "### `Noramlization`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bb4477-b41e-4993-8074-3a273943fe86",
   "metadata": {},
   "source": [
    "# Normalization: L1, L2, and Max Norm\n",
    "\n",
    "Normalization is a technique used to rescale feature vectors so that they can be compared on a common scale. Different normalization techniques serve different purposes depending on the needs of the machine learning algorithm. Below are three common types of normalization.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. L1 Normalization\n",
    "\n",
    "**When to Use**:  \n",
    "- Use when you want the sum of the absolute values of the features in a vector to equal 1.\n",
    "- Commonly used when dealing with sparse datasets or when using algorithms that rely on the size of vectors, like certain distance-based algorithms (e.g., KNN, text classification).\n",
    "\n",
    "**How to Use**:  \n",
    "L1 normalization scales each value by dividing it by the sum of absolute values in the vector. The formula is as follows:\n",
    "\n",
    "$ x' = \\frac{x}{\\sum |x|} $\n",
    "\n",
    "- `x` is the original value.\n",
    "- `x'` is the normalized value.\n",
    "- The sum of the absolute values of the vector elements equals 1.\n",
    "\n",
    "**Example**:  \n",
    "For a vector $ x = [3, 4, 5] $:\n",
    "\n",
    "1. $ \\sum |x| = 3 + 4 + 5 = 12 $\n",
    "\n",
    "$ x' = \\frac{x}{12} $\n",
    "\n",
    "The normalized values would be:\n",
    "\n",
    "$ x' = [0.25, 0.33, 0.42] $\n",
    "\n",
    "---\n",
    "\n",
    "## 2. L2 Normalization\n",
    "\n",
    "**When to Use**:  \n",
    "- Use when you want the length (Euclidean norm) of the vector to be 1.\n",
    "- Often used in text classification, image processing, and algorithms where the magnitude of vectors is important.\n",
    "\n",
    "**How to Use**:  \n",
    "L2 normalization scales each value in the vector by dividing it by the Euclidean norm (the square root of the sum of squared values). The formula is:\n",
    "\n",
    "$ x' = \\frac{x}{\\sqrt{x_1^2 + x_2^2 + \\dots + x_n^2}} = \\frac{x}{\\|x\\|_2} $\n",
    "\n",
    "- `x` is the original value.\n",
    "- `x'` is the normalized value.\n",
    "- The vector's L2 norm is the square root of the sum of squares of all values.\n",
    "\n",
    "**Example**:  \n",
    "For a vector $ x = [3, 4] $:\n",
    "\n",
    "1. Calculate the L2 norm:\n",
    "\n",
    "$ \\|x\\|_2 = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = 5 $\n",
    "\n",
    "2. Normalize the vector:\n",
    "\n",
    "$ x' = \\frac{x}{5} = [0.6, 0.8] $\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Max Normalization\n",
    "\n",
    "**When to Use**:  \n",
    "- Use when you want to scale values based on the maximum value in the feature.\n",
    "- Suitable when the feature’s range is unbounded, but you need to bring all values into proportion relative to the maximum value.\n",
    "- Can be useful in cases where you don’t want to distort the original data too much, but still want to normalize.\n",
    "\n",
    "**How to Use**:  \n",
    "Max normalization scales each value in the vector by dividing it by the maximum value in the vector. The formula is:\n",
    "\n",
    "$ x' = \\frac{x}{\\max(x)} $\n",
    "\n",
    "- `x` is the original value.\n",
    "- `x'` is the normalized value.\n",
    "- $ \\max(x) $ is the maximum value in the feature.\n",
    "\n",
    "**Example**:  \n",
    "For a feature $ x = [1, 3, 6, 9] $:\n",
    "\n",
    "1. Find the maximum value:\n",
    "\n",
    "$ \\max(x) = 9 $\n",
    "\n",
    "2. Normalize the vector:\n",
    "\n",
    "$ x' = \\frac{x}{9} $\n",
    "\n",
    "The normalized values would be:\n",
    "\n",
    "$ x' = [0.11, 0.33, 0.67, 1] $\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **L1 Normalization**: Scales so that the sum of absolute values equals 1. Use it for sparse data or when you want to minimize absolute differences.\n",
    "- **L2 Normalization**: Scales so that the vector's length equals 1. Use it for distance-based algorithms where vector direction matters.\n",
    "- **Max Normalization**: Scales so that the largest value in the vector equals 1. Use it when you want to maintain the relative proportions between values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "162ed758-3532-4763-a0c1-94f606c5fa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Normalized Data:\n",
      " [[0.57142857 0.14285714 0.28571429]\n",
      " [0.07692308 0.23076923 0.69230769]\n",
      " [0.33333333 0.46666667 0.2       ]]\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "data = np.array([[4, 1, 2], [1, 3, 9], [5, 7, 3]])\n",
    "\n",
    "# Apply L1 normalization\n",
    "l1_normalized_data = preprocessing.normalize(data, norm='l1')\n",
    "\n",
    "print(\"L1 Normalized Data:\\n\", l1_normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3202eb7b-d5ff-424c-a321-b2d16cff5aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Normalized Data:\n",
      " [[0.87287156 0.21821789 0.43643578]\n",
      " [0.10482848 0.31448545 0.94345635]\n",
      " [0.5488213  0.76834982 0.32929278]]\n"
     ]
    }
   ],
   "source": [
    "# Apply L2 normalization\n",
    "l2_normalized_data = preprocessing.normalize(data, norm='l2')\n",
    "\n",
    "print(\"L2 Normalized Data:\\n\", l2_normalized_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fdb3555f-01d0-44f1-8f1f-8b6a4d7b700b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Normalized Data:\n",
      " [[1.         0.25       0.5       ]\n",
      " [0.11111111 0.33333333 1.        ]\n",
      " [0.71428571 1.         0.42857143]]\n"
     ]
    }
   ],
   "source": [
    "# Apply Max normalization\n",
    "max_normalized_data = preprocessing.normalize(data, norm='max')\n",
    "\n",
    "print(\"Max Normalized Data:\\n\", max_normalized_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7063c608-f8c0-4050-a8b1-1a8ebb9fc18d",
   "metadata": {},
   "source": [
    "### `Binarization`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f5ee03-9760-4ce1-ad81-bc9b79613b1c",
   "metadata": {},
   "source": [
    "### Binarization\n",
    "\n",
    "**Binarization** is a process used to convert numerical data into two categories: typically represented by **0** and **1**. This is especially useful when working with machine learning or image processing tasks where you want to simplify data for better recognition and analysis.\n",
    "\n",
    "---\n",
    "\n",
    "#### How Binarization Works\n",
    "In binarization, each value is compared to a chosen threshold:\n",
    "- If a value is above the threshold, it is converted to **1**.\n",
    "- If a value is below the threshold, it is converted to **0**.\n",
    "\n",
    "This process transforms the original data into a **Boolean vector** (a sequence of 0s and 1s), which is easier to work with in many cases.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example in Image Processing\n",
    "In **digital image processing**, binarization is often used to convert a color or grayscale image into a **binary image**. A binary image consists of only two colors, typically **black** and **white**. This is useful for object recognition, shape analysis, or character recognition, where the goal is to separate objects (like letters or shapes) from the background.\n",
    "\n",
    "For example, in character recognition:\n",
    "- The image of a letter on a white background is converted into just black and white pixels.\n",
    "- The letter becomes **black** (1), and the background turns **white** (0).\n",
    "\n",
    "---\n",
    "\n",
    "#### Applications:\n",
    "- **Object Recognition**: Identifying specific shapes or objects within an image.\n",
    "- **Character Recognition**: Recognizing letters and numbers (like in OCR – Optical Character Recognition).\n",
    "- **Skeletonization**: Reducing an object to its simplest form for easier recognition later on.\n",
    "\n",
    "---\n",
    "\n",
    "By using binarization, it becomes easier to separate the object of interest (like a letter or shape) from the background, making further analysis and recognition more accurate and efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a3403e59-da80-4091-a62b-88075ba786a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_binarized = preprocessing.Binarizer(threshold=3).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9b38e3c2-db6d-4906-b5e5-5426d3ecd3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 0 1]\n",
      " [1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(data_binarized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b8976c-314b-44fd-aea0-e77e75a07927",
   "metadata": {},
   "source": [
    "### `One Hot Encoding`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb298fc6-29ee-4c97-971f-db4bace4f6ce",
   "metadata": {},
   "source": [
    "### What is One-Hot Encoding?\n",
    "\n",
    "One-hot encoding is a way to represent categorical data (non-numerical or integer categories) in a format that machine learning algorithms can understand. Instead of using the actual values (like 0, 1, 2), we represent each possible value with a separate column. \n",
    "\n",
    "---\n",
    "\n",
    "### Why One-Hot Encoding?\n",
    "\n",
    "When dealing with categorical data, machine learning algorithms might incorrectly assume that the categories have some kind of numerical importance or order (like 2 is greater than 1). One-hot encoding helps prevent this by creating new columns, where each column represents one possible value from the original data. \n",
    "\n",
    "Each category is then represented by a \"1\" in the corresponding column and \"0\" in all the other columns.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Data\n",
    "\n",
    "Let's say you have a small dataset with four rows (data points) and three columns (features), as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d9e32e7-35bf-4acc-83fa-85d146fac12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2]\n",
      " [0 2 3]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([[1, 1, 2], \n",
    "                 [0, 2, 3], \n",
    "                 [1, 0, 1], \n",
    "                 [0, 1, 0]])\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac12d56-2099-4942-8940-155aa74e1678",
   "metadata": {},
   "source": [
    "Each number in the array represents a category for a feature.\n",
    "\n",
    "### Analyzing the Values:\n",
    "- **First feature** has two possible values: `0` and `1`\n",
    "- **Second feature** has three possible values: `0`, `1`, and `2`\n",
    "- **Third feature** has four possible values: `0`, `1`, `2`, and `3`\n",
    "\n",
    "---\n",
    "\n",
    "So, to fully represent the data using one-hot encoding, we need a total of:\n",
    "- 2 columns for the first feature\n",
    "- 3 columns for the second feature\n",
    "- 4 columns for the third feature\n",
    "\n",
    "This gives us a total of **9 columns** to represent our original 3 features.\n",
    "\n",
    "---\n",
    "\n",
    "### How One-Hot Encoding Works:\n",
    "For each feature:\n",
    "- The **first feature** (values `0` and `1`) will be encoded into **2 columns**\n",
    "- The **second feature** (values `0`, `1`, `2`) will be encoded into **3 columns**\n",
    "- The **third feature** (values `0`, `1`, `2`, `3`) will be encoded into **4 columns**\n",
    "\n",
    "Each data point will be represented by a vector where one value is `1` (indicating the category), and the rest are `0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "553e9976-7a6a-4567-b3e6-8d3e2c1e6cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = preprocessing.OneHotEncoder()\n",
    "\n",
    "# Fit the encoder on our dataset\n",
    "encoder.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5955271b-e28f-4534-84f1-af021e8200d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 1. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Transform the data using the fitted encoder\n",
    "encoded_vector = encoder.transform([[1, 1, 2]]).toarray()\n",
    "\n",
    "# Display the encoded vector\n",
    "print(encoded_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbd401a2-09e7-41e3-a8eb-1ec7c120b040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 1. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Transform the data using the fitted encoder\n",
    "encoded_vector = encoder.transform([[1, 2, 2]]).toarray()\n",
    "\n",
    "# Display the encoded vector\n",
    "print(encoded_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "913bec2c-9b7e-4096-a33e-15d506b8984f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 1. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Transform the data using the fitted encoder\n",
    "encoded_vector = encoder.transform([[1, 2, 3]]).toarray()\n",
    "\n",
    "# Display the encoded vector\n",
    "print(encoded_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ba9deb-b13b-46c1-accc-bfcc49c79263",
   "metadata": {},
   "source": [
    "### Understanding the Output:\n",
    "- The **first feature** (value `1`) is represented by the second column being `1`: `[0, 1]`\n",
    "- The **second feature** (value `2`) is represented by the fifth column being `1`: `[0, 0, 1]`\n",
    "- The **third feature** (value `3`) is represented by the ninth column being `1`: `[0, 0, 0, 1]`\n",
    "\n",
    "Each feature is now represented in a way that machine learning algorithms can understand without implying any kind of numerical relationship between the categories.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "One-hot encoding converts categorical data into binary columns (`0`s and `1`s), where each column represents a specific category. This approach is essential when using machine learning models to avoid incorrectly implying any numerical relationship between categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0031683c-09ef-4a3c-8c9f-6d4bf633825f",
   "metadata": {},
   "source": [
    "### `Label Encoding`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab9a5e4-4b75-4049-971f-a6ffd5359d04",
   "metadata": {},
   "source": [
    "Label encoding is a method used to convert labels (the target/output in supervised learning) into a numerical format.\n",
    "\n",
    "## Why do we need Label Encoding?\n",
    "In supervised learning, the labels or outputs can be either **numbers** (which the algorithm can use directly) or **words** (like \"cat\", \"dog\", or \"apple\"). Since machine learning algorithms work with numbers, we need to convert these word labels into numbers.\n",
    "\n",
    "## How does Label Encoding work?\n",
    "Label encoding assigns a unique number to each word label. For example:\n",
    "\n",
    "- \"cat\" → 0\n",
    "- \"dog\" → 1\n",
    "- \"apple\" → 2\n",
    "\n",
    "So, if your labels were [\"cat\", \"dog\", \"apple\"], the algorithm would be able to use the encoded numbers [0, 1, 2] instead.\n",
    "\n",
    "Label encoding is useful when the labels are words, and you need to make them understandable for the machine learning algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76e7e220-c0ff-439a-8704-5d30836c4b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc3b6041-62ca-4921-9db6-37a8f4ab88fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_class = ['adidas','reebok','adidas','nike','reebok','skechers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da30053b-59a0-4006-aa77-7f5789866db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLass Mapping: \n",
      "adidas --> 0\n",
      "nike --> 1\n",
      "reebok --> 2\n",
      "skechers --> 3\n"
     ]
    }
   ],
   "source": [
    "label_encoder.fit(input_class)\n",
    "print(\"CLass Mapping: \")\n",
    "for i, item in enumerate(label_encoder.classes_):\n",
    "    print(item,\"-->\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6dd0ebae-1b2d-4760-aeee-414bbf032ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label = ['reebok', 'skechers', 'nike']\n",
      "Encoded Labels = [2, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "labels =['reebok','skechers','nike']\n",
    "encoded_labels = label_encoder.transform(labels)\n",
    "print(\"Label =\", labels)\n",
    "print(\"Encoded Labels =\", list(encoded_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1f85427-b411-4dcb-a9ca-9b0c92bbe4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Labels = [2, 1, 0, 3, 1]\n",
      "Decoded Labels = ['reebok' 'nike' 'adidas' 'skechers' 'nike']\n"
     ]
    }
   ],
   "source": [
    "encoded_labels = [2,1,0,3,1]\n",
    "decoded_labels = label_encoder.inverse_transform(encoded_labels)\n",
    "print(\"Encoded Labels =\", encoded_labels)\n",
    "print(\"Decoded Labels =\", decoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7eb474-c038-4d1d-acf2-34570301496d",
   "metadata": {},
   "source": [
    "## Label Encoding vs. One-Hot Encoding\n",
    "\n",
    "### Label Encoding:\n",
    "Label encoding is a method that transforms categorical data (like \"apple\", \"banana\", \"cherry\") into numbers. However, it can cause problems because it gives an order to the numbers (like 0, 1, 2), which may make the algorithm think that one label is \"greater\" or \"less\" than another. This can be an issue if the algorithm tries to do math with these numbers, which isn't appropriate for categories that don't have a natural order.\n",
    "\n",
    "### Why One-Hot Encoding is Better:\n",
    "One-hot encoding solves this problem by representing each category as a separate column with a binary value (0 or 1). This way, no order is implied, and each category is treated equally. It's like putting each category in its own \"box.\"\n",
    "\n",
    "### When to Use Label Encoding vs. One-Hot Encoding:\n",
    "In most cases, you don't have to use label encoding, especially when the categories don't have a natural order (like \"cat,\" \"dog,\" or \"apple\"). Using label encoding can mistakenly give the model a sense of hierarchy or order between categories, which could lead to incorrect predictions or assumptions.\n",
    "\n",
    "Instead, one-hot encoding is a better choice because it treats each category independently without implying any ranking or order. \n",
    "\n",
    "However, if your labels do have a meaningful order (like \"low,\" \"medium,\" \"high\"), then label encoding might be appropriate.\n",
    "\n",
    "### In Summary:\n",
    "- **Use one-hot encoding** for categorical data without any natural order.\n",
    "- **Use label encoding** if the categories have a clear order or ranking.\n",
    "\n",
    "### Advantages and Disadvantages:\n",
    "\n",
    "- **Advantage:** One-hot encoding is **binary**, not ordinal, and the categories are completely separate in the data.\n",
    "  \n",
    "- **Disadvantage:** If you have many categories (high cardinality), the number of columns (or feature space) can become very large, which can make the data harder to manage.\n",
    "\n",
    "### In short:\n",
    "One-hot encoding is great for preventing problems caused by giving categories an order, but it can create a lot of new columns if there are many categories.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
