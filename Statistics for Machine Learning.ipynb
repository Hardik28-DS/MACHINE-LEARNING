{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8edbe7b8-fed8-4543-81f1-7a1d2eeb78ef",
   "metadata": {},
   "source": [
    "## Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a00b80-8415-4341-b26e-9078fd5ff573",
   "metadata": {},
   "source": [
    "### **Hypothesis Testing**\n",
    "Hypothesis testing is a statistical method used to draw conclusions about a population based on sample data. It involves formulating two contrasting hypotheses:\n",
    "- **Null Hypothesis ($H_0$)**: Assumes there is no relationship, effect, or difference in the population.\n",
    "- **Alternative Hypothesis ($H_a$)**: Suggests the presence of a relationship, effect, or difference.\n",
    "\n",
    "The goal is to determine whether there is enough evidence to reject the null hypothesis in favor of the alternative. Hypothesis testing plays a crucial role in statistical analysis and is often applied in predictive modeling and feature selection.\n",
    "\n",
    "### **P-Value**\n",
    "The **P-value** indicates the probability of observing a test statistic as extreme as, or more extreme than, the one obtained, assuming the null hypothesis ($H_0$) is true. It serves as a key measure for decision-making in hypothesis testing:\n",
    "- **$p < 0.05$**: The null hypothesis is rejected, and the alternative hypothesis is supported. \n",
    "- **$p > 0.05$**: There is insufficient evidence to reject the null hypothesis.\n",
    "\n",
    "A p-value threshold of 0.05 is commonly used, but this threshold may vary depending on the context. In machine learning, p-values are often used to test the relevance of independent variables, helping to identify which features have a significant impact on the model's predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e202d87-fb86-4530-9905-44c72eeb9190",
   "metadata": {},
   "source": [
    "### **Steps in Hypothesis Testing**\n",
    "The process of hypothesis testing involves the following key steps:\n",
    "\n",
    "1. **Formulate the Null Hypothesis ($H_0$)**:  \n",
    "   Assume a null hypothesis that represents the default position (for example, no difference, no significance, or no relationship). The null hypothesis typically suggests that there is no anomaly or pattern present in the data.\n",
    "\n",
    "2. **Collect a Sample**:  \n",
    "   Gather a representative sample from the population to test the hypothesis.\n",
    "\n",
    "3. **Compute the Test Statistic**:  \n",
    "   Use the sample data to calculate a test statistic (like z-statistic, t-statistic, or chi-square) to measure the degree of agreement between the sample and the null hypothesis.\n",
    "\n",
    "4. **Make a Decision**:  \n",
    "   Based on the value of the test statistic and the corresponding p-value, decide whether to **reject** or **fail to reject** the null hypothesis. If the p-value is less than a specified threshold (typically 0.05), the null hypothesis is rejected in favor of the alternative hypothesis ($H_a$).\n",
    "\n",
    "These steps form the basis for hypothesis testing, which is widely used in statistical analysis and machine learning for feature selection, model validation, and evaluating relationships between variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf7382-e266-45bd-9ab1-39ac91fed8aa",
   "metadata": {},
   "source": [
    "### **Example of Hypothesis Testing**\n",
    "A soft drink manufacturer claims that every bottle of their soda contains **at least 500 ml** of liquid. You suspect that the actual volume might be less than 500 ml. To verify this, you collect a random sample of 40 soda bottles and measure the volume in each. The sample shows an **average volume of 495 ml** with a **sample standard deviation of 8 ml**. Given a **significance level of 0.05**, can you reject the manufacturer's claim?\n",
    "\n",
    "### **Step 1: Formulate Hypotheses**\n",
    "- **Null Hypothesis ($H_0$):** The average volume of soda bottles is at least 500 ml.  \n",
    "  \\[\n",
    "  H_0: \\mu \\geq 500\n",
    "  \\]\n",
    "- **Alternative Hypothesis ($H_a$):** The average volume of soda bottles is less than 500 ml.  \n",
    "  \\[\n",
    "  H_a: \\mu < 500\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Collect the Sample**\n",
    "- Sample size ($n$) = 40  \n",
    "- Sample mean ($\\bar{x}$) = 495 ml  \n",
    "- Sample standard deviation ($s$) = 8 ml  \n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Calculate the Test Statistic**\n",
    "To calculate the **t-statistic**, we use the formula:  \n",
    "\\[\n",
    "t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n",
    "\\]\n",
    "Where:  \n",
    "- $\\bar{x}$ = Sample mean (495)  \n",
    "- $\\mu_0$ = Population mean under $H_0$ (500)  \n",
    "- $s$ = Sample standard deviation (8)  \n",
    "- $n$ = Sample size (40)  \n",
    "\n",
    "\\[\n",
    "t = \\frac{495 - 500}{8 / \\sqrt{40}} = \\frac{-5}{8 / 6.32} = \\frac{-5}{1.265} \\approx -3.95\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Make a Decision**\n",
    "- **Significance level ($\\alpha$) = 0.05**  \n",
    "- Find the critical t-value for a one-tailed test with **n-1 = 40 - 1 = 39 degrees of freedom**.  \n",
    "  Using a t-table, the critical t-value for $\\alpha = 0.05$ and **df = 39** is **-1.685**.  \n",
    "  Since **$t = -3.95$** is less than **$-1.685$**, we reject the null hypothesis.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "Since the t-statistic **(-3.95)** lies in the rejection region (beyond -1.685), we reject the null hypothesis ($H_0$).  \n",
    "**Conclusion:** There is sufficient evidence at the 0.05 significance level to reject the claim that each soda bottle contains at least 500 ml of liquid. The data suggests that the actual average volume is less than 500 ml.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "129c2ce1-3556-4b2a-a949-b146e2d92ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Statistic: 3.95\n",
      "Critical value from t-table: -1.685\n",
      "Lower tail p-value from t-table 0.00015761816112839098\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "xbar = 500\n",
    "mu0 = 495\n",
    "s = 8\n",
    "n = 40\n",
    "\n",
    "# Test Statistic\n",
    "t_smple  = (xbar-mu0)/(s/np.sqrt(float(n))); print (\"Test Statistic:\",round(t_smple,2)) \n",
    "\n",
    "# Critical value from t-table \n",
    "alpha = 0.05 \n",
    "t_alpha = stats.t.ppf(alpha,n-1); print (\"Critical value from t-table:\",round(t_alpha,3))  \n",
    "\n",
    "#Lower tail p-value from t-table \n",
    "p_val = stats.t.sf(np.abs(t_smple), n-1); print (\"Lower tail p-value from t-table\", p_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db26202-2581-40be-b2cb-b964f8ef949e",
   "metadata": {},
   "source": [
    "### **Type I and Type II Errors in Hypothesis Testing**\n",
    "\n",
    "When conducting hypothesis testing, we make decisions about the null hypothesis ($H_0$) based on sample data, not the entire population. As a result, there are two possible errors that can occur in this decision-making process:\n",
    "\n",
    "1. **Type I Error ($\\alpha$)**:  \n",
    "   Occurs when we **reject a true null hypothesis**.  \n",
    "   In other words, we conclude that an effect or difference exists when, in reality, it does not.  \n",
    "   This is also known as a **\"false positive\"**.  \n",
    "   Example: \n",
    "   - Suppose a drug company claims that a new medication has no side effects (null hypothesis: no side effects). \n",
    "   - If a study incorrectly finds evidence of side effects when, in reality, there are none, this is a Type I error.\n",
    "\n",
    "2. **Type II Error ($\\beta$)**:  \n",
    "   Occurs when we **fail to reject a false null hypothesis**.  \n",
    "   In other words, we conclude that there is no effect or difference when, in reality, there is one.  \n",
    "   This is also known as a **\"false negative\"**.  \n",
    "   Example: \n",
    "   - Suppose a medical test is being conducted to detect a disease.  \n",
    "   - If the test fails to detect the disease in a patient who actually has it, this is a Type II error.  \n",
    "   - This could happen if the sample size is too small or if the test is not sensitive enough.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example of Type I and Type II Errors**\n",
    "\n",
    "**Scenario**: A company claims that their bottled water has exactly **500 ml of water** on average.  \n",
    "- **Null Hypothesis ($H_0$):** The average volume of water in the bottle is 500 ml.  \n",
    "- **Alternative Hypothesis ($H_a$):** The average volume of water in the bottle is not 500 ml.  \n",
    "\n",
    "| **Decision**               | **Reality (H0 is True)**                  | **Reality (H0 is False)**                    |\n",
    "|--------------------------|--------------------------------------------|---------------------------------------------|\n",
    "| **Reject $H_0$**           | **Type I Error** (False positive) – We conclude that the bottle does not have 500 ml, but it actually does. | **Correct Decision** – We correctly identify that the bottle does not have 500 ml. |\n",
    "| **Fail to Reject $H_0$**   | **Correct Decision** – We correctly conclude that the bottle has 500 ml.  | **Type II Error** (False negative) – We fail to detect that the bottle does not have 500 ml. |\n",
    "\n",
    "---\n",
    "\n",
    "### **Reducing Type I and Type II Errors**\n",
    "- **To reduce Type I Error ($\\alpha$)**, decrease the significance level (e.g., from 0.05 to 0.01).  \n",
    "- **To reduce Type II Error ($\\beta$)**, increase the sample size or increase the test's power by using a larger, more representative dataset.  \n",
    "\n",
    "---\n",
    "\n",
    "In summary:  \n",
    "- **Type I Error**: False positive – rejecting a true null hypothesis.  \n",
    "- **Type II Error**: False negative – failing to reject a false null hypothesis.  \n",
    "Both errors have consequences, and balancing them is essential for good decision-making in hypothesis testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11df3b5d-9df5-42fb-8c9d-4a17db6335e5",
   "metadata": {},
   "source": [
    "### **What is a Normal Distribution?**\n",
    "A **normal distribution** (also known as a Gaussian distribution or bell curve) is a probability distribution that is symmetric about its mean. It represents the distribution of many natural phenomena and datasets.\n",
    "\n",
    "The **probability density function (PDF)** for a normal distribution is given by:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $x$: A data point  \n",
    "- $\\mu$: Mean (center) of the distribution  \n",
    "- $\\sigma$: Standard deviation (spread or width of the curve)  \n",
    "- $\\pi$ and $e$: Mathematical constants  \n",
    "\n",
    "---\n",
    "\n",
    "### **Key Characteristics of Normal Distribution**\n",
    "1. **Symmetry**: The distribution is symmetric around the mean.  \n",
    "2. **Mean = Median = Mode**: These three measures of central tendency are identical.  \n",
    "3. **Bell Shape**: Most data points are concentrated around the mean, with fewer as you move away.  \n",
    "4. **Empirical Rule (68-95-99.7 Rule)**:  \n",
    "   - 68% of data falls within 1 standard deviation of the mean ($\\mu \\pm \\sigma$).  \n",
    "   - 95% of data falls within 2 standard deviations ($\\mu \\pm 2\\sigma$).  \n",
    "   - 99.7% of data falls within 3 standard deviations ($\\mu \\pm 3\\sigma$).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Why is Data Often Normally Distributed?**\n",
    "\n",
    "1. **Central Limit Theorem (CLT)**:  \n",
    "   - The **Central Limit Theorem (CLT)** states that when independent random variables are added, their sum tends toward a normal distribution, regardless of the original distributions of the variables, as the sample size becomes large.  \n",
    "   - In simpler terms, many small, independent factors influence most phenomena, causing the data to naturally form a normal distribution.  \n",
    "\n",
    "2. **Natural Phenomena**:  \n",
    "   - Many real-world processes result from random interactions of multiple factors.  \n",
    "   - **Examples**:  \n",
    "     - Heights of people  \n",
    "     - Measurement errors  \n",
    "     - Blood pressure  \n",
    "     - Exam scores  \n",
    "\n",
    "3. **Maximum Entropy Principle**:  \n",
    "   - Among all distributions with a given mean and variance, the normal distribution has the maximum entropy (is the most \"unbiased\").  \n",
    "   - This makes it a natural choice for many random processes.  \n",
    "\n",
    "4. **Measurement and Noise**:  \n",
    "   - Data often arises from measurements, and random errors in measurements tend to be normally distributed.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Examples of Normally Distributed Data**\n",
    "1. **Biological Data**:  \n",
    "   - Heights, weights, and blood pressure in a population.  \n",
    "\n",
    "2. **Finance**:  \n",
    "   - Stock price changes over small time intervals.  \n",
    "\n",
    "3. **Physics**:  \n",
    "   - Measurement errors in scientific experiments.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Why is Normal Distribution Useful?**\n",
    "1. **Statistical Modeling**:  \n",
    "   - Many statistical tests (e.g., t-tests, ANOVA) assume that the data is normally distributed.  \n",
    "\n",
    "2. **Predictive Modeling**:  \n",
    "   - In machine learning, features are often transformed to approximate normality to improve model performance.  \n",
    "\n",
    "3. **Real-World Interpretation**:  \n",
    "   - Allows probabilistic reasoning (e.g., \"What is the likelihood that a student scores above 90?\").  \n",
    "\n",
    "---\n",
    "\n",
    "This markdown provides a clear, structured explanation of the **Normal Distribution**, including the probability density function, key properties, reasons for normality, and examples of its occurrence in real-world data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bf6ba8-bfb2-47c2-a820-abe13f0db852",
   "metadata": {},
   "source": [
    "### **What is a Chi-Square Test?**\n",
    "The **Chi-Square Test** is a statistical test used to determine if there is a significant relationship between categorical variables. It compares the observed data with the data expected under the assumption that the variables are independent.\n",
    "\n",
    "There are two main types of Chi-Square tests:\n",
    "1. **Chi-Square Test of Independence**: Tests whether two categorical variables are independent of each other.\n",
    "2. **Chi-Square Goodness of Fit Test**: Tests how well an observed distribution fits a specific theoretical distribution.\n",
    "\n",
    "---\n",
    "\n",
    "### **Chi-Square Test Formula**\n",
    "The Chi-Square statistic ($\\chi^2$) is calculated as:\n",
    "\n",
    "$$\n",
    "\\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $O_i$: Observed frequency of category $i$  \n",
    "- $E_i$: Expected frequency of category $i$  \n",
    "- $\\sum$: Summation over all categories  \n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use the Chi-Square Test?**\n",
    "- When you want to check if two categorical variables are related (Chi-Square Test of Independence).  \n",
    "- When you want to check if the observed data fits a theoretical distribution (Goodness of Fit Test).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Example of Chi-Square Test of Independence**\n",
    "\n",
    "#### **Scenario**:\n",
    "A school principal wants to know if **gender (Male/Female)** is related to students' **preference for online vs offline classes**.  \n",
    "He collects the following data from 100 students:\n",
    "\n",
    "| **Gender** | **Prefers Online** | **Prefers Offline** | **Total** |\n",
    "|------------|-------------------|---------------------|------------|\n",
    "| **Male**   | 30                 | 20                  | 50         |\n",
    "| **Female** | 10                 | 40                  | 50         |\n",
    "| **Total**  | 40                 | 60                  | 100        |\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Formulate Hypotheses**\n",
    "- **Null Hypothesis ($H_0$)**: Gender and preference for online/offline classes are independent.  \n",
    "- **Alternative Hypothesis ($H_a$)**: Gender and preference for online/offline classes are not independent.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Calculate Expected Frequencies**\n",
    "The expected frequency ($E_i$) for each cell is calculated using the formula:  \n",
    "\n",
    "$$\n",
    "E_{ij} = \\frac{(row\\ total) \\cdot (column\\ total)}{grand\\ total}\n",
    "$$\n",
    "\n",
    "| **Gender** | **Prefers Online** ($E_{11}$) | **Prefers Offline** ($E_{12}$) | **Total** |\n",
    "|------------|-----------------------------|---------------------------------|------------|\n",
    "| **Male**   | $E_{11} = \\frac{50 \\cdot 40}{100} = 20$ | $E_{12} = \\frac{50 \\cdot 60}{100} = 30$ | 50         |\n",
    "| **Female** | $E_{21} = \\frac{50 \\cdot 40}{100} = 20$ | $E_{22} = \\frac{50 \\cdot 60}{100} = 30$ | 50         |\n",
    "| **Total**  | 40                            | 60                             | 100        |\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Calculate Chi-Square Statistic**\n",
    "Use the Chi-Square formula:  \n",
    "\n",
    "$$\n",
    "\\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}\n",
    "$$\n",
    "\n",
    "| **Gender** | **Prefers Online** | **Prefers Offline** | **Observed (O)** | **Expected (E)** | **$(O - E)^2 / E$** |\n",
    "|------------|-------------------|---------------------|-----------------|------------------|----------------------|\n",
    "| **Male**   | 30                 | 20                  | 30 (O)          | 20 (E)           | $\\frac{(30 - 20)^2}{20} = 5$ |\n",
    "| **Male**   | 20                 | 30                  | 20 (O)          | 30 (E)           | $\\frac{(20 - 30)^2}{30} = 3.33$ |\n",
    "| **Female** | 10                 | 40                  | 10 (O)          | 20 (E)           | $\\frac{(10 - 20)^2}{20} = 5$ |\n",
    "| **Female** | 40                 | 30                  | 40 (O)          | 30 (E)           | $\\frac{(40 - 30)^2}{30} = 3.33$ |\n",
    "\n",
    "Total Chi-Square Statistic:  \n",
    "$$\n",
    "\\chi^2 = 5 + 3.33 + 5 + 3.33 = 16.66\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Determine the Degrees of Freedom (df)**\n",
    "The degrees of freedom for a Chi-Square test of independence is:  \n",
    "\n",
    "$$\n",
    "df = (rows - 1) \\cdot (columns - 1)\n",
    "$$\n",
    "\n",
    "In our case:  \n",
    "$$\n",
    "df = (2 - 1) \\cdot (2 - 1) = 1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Make a Decision**\n",
    "1. **Significance level ($\\alpha$)** = 0.05  \n",
    "2. **Critical value** for $\\chi^2$ with **df = 1** and $\\alpha = 0.05$ (from Chi-Square table) = **3.841**  \n",
    "3. Our calculated $\\chi^2 = 16.66$ is greater than the critical value (3.841).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Step 6: Conclusion**\n",
    "Since $\\chi^2 = 16.66$ is greater than the critical value of 3.841, we **reject the null hypothesis** ($H_0$).  \n",
    "**Conclusion**: There is sufficient evidence to suggest that there is a relationship between **gender** and **preference for online/offline classes**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "1. The **Chi-Square Test** checks the relationship between categorical variables.  \n",
    "2. It compares **observed** and **expected frequencies** in a contingency table.  \n",
    "3. The test can be used for:  \n",
    "   - **Independence Test**: To see if two variables are related.  \n",
    "   - **Goodness of Fit**: To check if observed data fits a particular distribution.  \n",
    "4. It is useful in fields like marketing, social sciences, and biological research.  \n",
    "\n",
    "This markdown provides a clear, structured explanation of the Chi-Square Test with a **step-by-step example** using properly formatted mathematical expressions for Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2180655e-5a59-45ba-b2f1-aa7b20d80339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from scipy import stats \n",
    " \n",
    "survey = pd.read_csv(\"data/survey.csv\")   \n",
    " \n",
    "# Tabulating 2 variables with row & column variables respectively \n",
    "survey_tab = pd.crosstab(survey.Smoke, survey.Exer, margins = True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c65d2139-1de0-43f2-8e66-66bab2c8cac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Wr.Hnd</th>\n",
       "      <th>NW.Hnd</th>\n",
       "      <th>W.Hnd</th>\n",
       "      <th>Fold</th>\n",
       "      <th>Pulse</th>\n",
       "      <th>Clap</th>\n",
       "      <th>Exer</th>\n",
       "      <th>Smoke</th>\n",
       "      <th>Height</th>\n",
       "      <th>M.I</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>18.5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Right</td>\n",
       "      <td>R on L</td>\n",
       "      <td>92.0</td>\n",
       "      <td>Left</td>\n",
       "      <td>Some</td>\n",
       "      <td>Never</td>\n",
       "      <td>173.0</td>\n",
       "      <td>Metric</td>\n",
       "      <td>18.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>19.5</td>\n",
       "      <td>20.5</td>\n",
       "      <td>Left</td>\n",
       "      <td>R on L</td>\n",
       "      <td>104.0</td>\n",
       "      <td>Left</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Regul</td>\n",
       "      <td>177.8</td>\n",
       "      <td>Imperial</td>\n",
       "      <td>17.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>Right</td>\n",
       "      <td>L on R</td>\n",
       "      <td>87.0</td>\n",
       "      <td>Neither</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Occas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>18.8</td>\n",
       "      <td>18.9</td>\n",
       "      <td>Right</td>\n",
       "      <td>R on L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neither</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Never</td>\n",
       "      <td>160.0</td>\n",
       "      <td>Metric</td>\n",
       "      <td>20.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Right</td>\n",
       "      <td>Neither</td>\n",
       "      <td>35.0</td>\n",
       "      <td>Right</td>\n",
       "      <td>Some</td>\n",
       "      <td>Never</td>\n",
       "      <td>165.0</td>\n",
       "      <td>Metric</td>\n",
       "      <td>23.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>Female</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Right</td>\n",
       "      <td>L on R</td>\n",
       "      <td>85.0</td>\n",
       "      <td>Right</td>\n",
       "      <td>Some</td>\n",
       "      <td>Never</td>\n",
       "      <td>165.1</td>\n",
       "      <td>Imperial</td>\n",
       "      <td>17.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>Female</td>\n",
       "      <td>18.5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Right</td>\n",
       "      <td>L on R</td>\n",
       "      <td>88.0</td>\n",
       "      <td>Right</td>\n",
       "      <td>Some</td>\n",
       "      <td>Never</td>\n",
       "      <td>160.0</td>\n",
       "      <td>Metric</td>\n",
       "      <td>16.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>Female</td>\n",
       "      <td>17.5</td>\n",
       "      <td>16.5</td>\n",
       "      <td>Right</td>\n",
       "      <td>R on L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Right</td>\n",
       "      <td>Some</td>\n",
       "      <td>Never</td>\n",
       "      <td>170.0</td>\n",
       "      <td>Metric</td>\n",
       "      <td>18.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>Male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.5</td>\n",
       "      <td>Right</td>\n",
       "      <td>R on L</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Right</td>\n",
       "      <td>Some</td>\n",
       "      <td>Never</td>\n",
       "      <td>183.0</td>\n",
       "      <td>Metric</td>\n",
       "      <td>17.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>Female</td>\n",
       "      <td>17.6</td>\n",
       "      <td>17.3</td>\n",
       "      <td>Right</td>\n",
       "      <td>R on L</td>\n",
       "      <td>85.0</td>\n",
       "      <td>Right</td>\n",
       "      <td>Freq</td>\n",
       "      <td>Never</td>\n",
       "      <td>168.5</td>\n",
       "      <td>Metric</td>\n",
       "      <td>17.750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>237 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sex  Wr.Hnd  NW.Hnd  W.Hnd     Fold  Pulse     Clap  Exer  Smoke  \\\n",
       "0    Female    18.5    18.0  Right   R on L   92.0     Left  Some  Never   \n",
       "1      Male    19.5    20.5   Left   R on L  104.0     Left   NaN  Regul   \n",
       "2      Male    18.0    13.3  Right   L on R   87.0  Neither   NaN  Occas   \n",
       "3      Male    18.8    18.9  Right   R on L    NaN  Neither   NaN  Never   \n",
       "4      Male    20.0    20.0  Right  Neither   35.0    Right  Some  Never   \n",
       "..      ...     ...     ...    ...      ...    ...      ...   ...    ...   \n",
       "232  Female    18.0    18.0  Right   L on R   85.0    Right  Some  Never   \n",
       "233  Female    18.5    18.0  Right   L on R   88.0    Right  Some  Never   \n",
       "234  Female    17.5    16.5  Right   R on L    NaN    Right  Some  Never   \n",
       "235    Male    21.0    21.5  Right   R on L   90.0    Right  Some  Never   \n",
       "236  Female    17.6    17.3  Right   R on L   85.0    Right  Freq  Never   \n",
       "\n",
       "     Height       M.I     Age  \n",
       "0     173.0    Metric  18.250  \n",
       "1     177.8  Imperial  17.583  \n",
       "2       NaN       NaN  16.917  \n",
       "3     160.0    Metric  20.333  \n",
       "4     165.0    Metric  23.667  \n",
       "..      ...       ...     ...  \n",
       "232   165.1  Imperial  17.667  \n",
       "233   160.0    Metric  16.917  \n",
       "234   170.0    Metric  18.583  \n",
       "235   183.0    Metric  17.167  \n",
       "236   168.5    Metric  17.750  \n",
       "\n",
       "[237 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2687c52b-690e-4b31-ae26-1be21005b5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Exer</th>\n",
       "      <th>Freq</th>\n",
       "      <th>Some</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Smoke</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Heavy</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Never</th>\n",
       "      <td>87</td>\n",
       "      <td>84</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Occas</th>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Regul</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>115</td>\n",
       "      <td>98</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Exer   Freq  Some  All\n",
       "Smoke                 \n",
       "Heavy     7     3   10\n",
       "Never    87    84  171\n",
       "Occas    12     4   16\n",
       "Regul     9     7   16\n",
       "All     115    98  213"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "941c4df4-622e-455b-bd96-10d6a4a59088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Exer</th>\n",
       "      <th>Freq</th>\n",
       "      <th>Some</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Smoke</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Heavy</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Never</th>\n",
       "      <td>87</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Occas</th>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Regul</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Exer   Freq  Some\n",
       "Smoke            \n",
       "Heavy     7     3\n",
       "Never    87    84\n",
       "Occas    12     4\n",
       "Regul     9     7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating observed table for analysis \n",
    "observed = survey_tab.iloc[0:4, 0:2]\n",
    "observed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5405f0-5795-400d-8318-dbdfc7f5e65e",
   "metadata": {},
   "source": [
    "### If p-value < 0.05, there is a strong dependency between two variables, whereas if p-value > 0.05, there is no dependency between the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9c44c4b-380c-47fc-bc4d-e94608d2ab80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value is:  0.206\n"
     ]
    }
   ],
   "source": [
    "contg = stats.chi2_contingency(observed= observed) \n",
    "p_value = round(contg[1],3) \n",
    "print (\"P-value is: \",p_value) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4e31a1-1cbf-4d61-9b4b-cf404d37fa88",
   "metadata": {},
   "source": [
    "# **Analysis of Variance (ANOVA)**\n",
    "\n",
    "## **What is ANOVA?**\n",
    "**ANOVA** (Analysis of Variance) is a statistical method used to determine if there is a **significant difference in the means** of two or more groups. It examines the amount of variation **between groups** compared to the amount of variation **within groups**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why Use ANOVA?**\n",
    "- **When you have more than two groups** to compare.  \n",
    "- When you want to identify if there is a difference between multiple group means.  \n",
    "- ANOVA prevents the need to conduct multiple **t-tests**, reducing the chance of a Type I error.\n",
    "\n",
    "---\n",
    "\n",
    "## **When to Use ANOVA?**\n",
    "- When you have a **categorical independent variable** (e.g., Diet Type, Treatment Group).  \n",
    "- When you have a **continuous dependent variable** (e.g., Exam Score, Weight, Revenue).  \n",
    "\n",
    "---\n",
    "\n",
    "## **Types of ANOVA**\n",
    "1. **One-Way ANOVA**: Used to compare the means of multiple groups for one factor.  \n",
    "2. **Two-Way ANOVA**: Used to compare the means across multiple factors (e.g., gender and education level).  \n",
    "3. **Repeated Measures ANOVA**: Used to analyze repeated measurements of the same group over time.  \n",
    "\n",
    "---\n",
    "\n",
    "## **How Does ANOVA Work?**\n",
    "ANOVA works by comparing the variance **between groups** and **within groups**.  \n",
    "- If the variance **between groups** is significantly larger than the variance **within groups**, it indicates that at least one group is different.  \n",
    "- It computes an **F-statistic** to determine the ratio of **between-group variance to within-group variance**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Key Terms**\n",
    "- **Between-Group Variance**: Measures the difference between the group means.  \n",
    "- **Within-Group Variance**: Measures the variation within each group.  \n",
    "- **F-statistic**: Ratio of Between-Group Variance to Within-Group Variance.  \n",
    "- **p-value**: The probability of observing the data if the null hypothesis is true.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Mathematical Formulas**\n",
    "\n",
    "### **1. Total Variation (SST)**\n",
    "$SST = \\sum_{i=1}^{N} (X_i - \\bar{X})^2$\n",
    "Where:  \n",
    "- $X_i$ = Each observation  \n",
    "- $\\bar{X}$ = Grand mean of all observations  \n",
    "- $N$ = Total number of data points  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. Between-Group Variation (SSB)**\n",
    "$SSB = \\sum_{j=1}^{k} n_j (\\bar{X}_j - \\bar{X})^2$\n",
    "Where:  \n",
    "- $n_j$ = Number of samples in group $j$  \n",
    "- $\\bar{X}_j$ = Mean of group $j$  \n",
    "- $\\bar{X}$ = Grand mean  \n",
    "\n",
    "---\n",
    "\n",
    "### **3. Within-Group Variation (SSW)**\n",
    "$SSW = \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (X_{ij} - \\bar{X}_j)^2$\n",
    "Where:  \n",
    "- $X_{ij}$ = Data point $i$ in group $j$  \n",
    "- $\\bar{X}_j$ = Mean of group $j$  \n",
    "\n",
    "---\n",
    "\n",
    "### **4. F-statistic**\n",
    "$F = \\frac{\\text{MSB}}{\\text{MSW}}$\n",
    "Where:  \n",
    "- **MSB** = Mean Square Between Groups = $\\frac{SSB}{k - 1}$  \n",
    "- **MSW** = Mean Square Within Groups = $\\frac{SSW}{N - k}$  \n",
    "- $k$ = Number of groups  \n",
    "- $N$ = Total number of data points  \n",
    "\n",
    "---\n",
    "\n",
    "## **Hypotheses for ANOVA**\n",
    "- **Null Hypothesis ($H_0$)**: All group means are equal.  \n",
    "  $H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\cdots = \\mu_k$\n",
    "- **Alternative Hypothesis ($H_1$)**: At least one of the group means is different.  \n",
    "  $H_1: \\text{At least one of the group means is different.}$\n",
    "\n",
    "---\n",
    "\n",
    "## **Example (One-Way ANOVA)**\n",
    "**Problem**: A researcher wants to know if 3 different diets affect weight loss.  \n",
    "The weights lost by participants on each diet are:  \n",
    "\n",
    "| **Diet A** | **Diet B** | **Diet C** |\n",
    "|------------|------------|------------|\n",
    "| 4          | 3          | 6          |\n",
    "| 5          | 7          | 8          |\n",
    "| 6          | 4          | 7          |\n",
    "| 7          | 5          | 5          |\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Calculate Group Means ($\\bar{X}_j$)**\n",
    "$\\bar{X}_A = \\frac{4 + 5 + 6 + 7}{4} = 5.5, \\quad \\bar{X}_B = \\frac{3 + 7 + 4 + 5}{4} = 4.75, \\quad \\bar{X}_C = \\frac{6 + 8 + 7 + 5}{4} = 6.5$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Calculate Grand Mean ($\\bar{X}$)**\n",
    "$\\bar{X} = \\frac{4 + 5 + 6 + 7 + 3 + 7 + 4 + 5 + 6 + 8 + 7 + 5}{12} = 5.583$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Calculate Between-Group Variation (SSB)**\n",
    "$SSB = 4(5.5 - 5.583)^2 + 4(4.75 - 5.583)^2 + 4(6.5 - 5.583)^2$\n",
    "$SSB = 0.0276 + 2.776 + 3.34 = 6.14$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Calculate Within-Group Variation (SSW)**\n",
    "$SSW = \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (X_{ij} - \\bar{X}_j)^2$\n",
    "For each group:  \n",
    "- **Group A**: $(4 - 5.5)^2 + (5 - 5.5)^2 + (6 - 5.5)^2 + (7 - 5.5)^2$  \n",
    "- **Group B**: $(3 - 4.75)^2 + (7 - 4.75)^2 + (4 - 4.75)^2 + (5 - 4.75)^2$  \n",
    "- **Group C**: $(6 - 6.5)^2 + (8 - 6.5)^2 + (7 - 6.5)^2 + (5 - 6.5)^2$  \n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Calculate F-Statistic**\n",
    "$F = \\frac{\\text{MSB}}{\\text{MSW}}$\n",
    "Where:  \n",
    "- **MSB** = $\\frac{SSB}{k - 1}$  \n",
    "- **MSW** = $\\frac{SSW}{N - k}$  \n",
    "\n",
    "---\n",
    "\n",
    "### **When Do You Reject the Null Hypothesis?**\n",
    "- If the **F-statistic** is larger than the **critical F-value**, reject the null hypothesis.  \n",
    "- If the **p-value** is smaller than the significance level (e.g., 0.05), reject the null hypothesis.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "- **ANOVA** is used to compare the means of multiple groups.  \n",
    "- It prevents multiple t-tests, reducing the chance of a **Type I error**.  \n",
    "- If the p-value is **less than 0.05**, we reject the null hypothesis, indicating that at least one of the means is different.  \n",
    "- One-way ANOVA is used for one independent variable, while two-way ANOVA is used for two independent variables.  \n",
    "\n",
    "If you'd like help with a **Python implementation** of ANOVA using **`scipy.stats.f_oneway()`** or **`statsmodels`**, let me know!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b577f142-c9cf-4bf2-a014-bb659cbac8ac",
   "metadata": {},
   "source": [
    "# **Decision to Reject the Null Hypothesis in ANOVA**\n",
    "\n",
    "## **1️⃣ When to Reject the Null Hypothesis?**\n",
    "- If the **F-statistic** is larger than the **critical F-value**, we **reject the null hypothesis**.  \n",
    "- If the **p-value** is **less than 0.05** (or your chosen significance level), we **reject the null hypothesis**.  \n",
    "\n",
    "The null hypothesis ($H_0$) for ANOVA is that **all group means are equal**.  \n",
    "If we reject $H_0$, it means at least one of the group means is different.\n",
    "\n",
    "---\n",
    "\n",
    "## **2️⃣ Example Setup**\n",
    "We want to determine if three different diets affect weight loss.  \n",
    "Here is the summary of the key data from the earlier example.\n",
    "\n",
    "| **Diet**  | **Group Mean** ($\\bar{X}_j$) |\n",
    "|-----------|------------------------------|\n",
    "| **Diet A**| 5.5                            |\n",
    "| **Diet B**| 4.75                           |\n",
    "| **Diet C**| 6.5                            |\n",
    "\n",
    "**Grand Mean** ($\\bar{X}$) = 5.583  \n",
    "\n",
    "- **Between-Group Variation (SSB)** = 6.14  \n",
    "- **Within-Group Variation (SSW)** = Calculated using individual deviations from group means.  \n",
    "\n",
    "---\n",
    "\n",
    "## **3️⃣ Calculate the F-statistic**\n",
    "\n",
    "To calculate the F-statistic, use the formula:  \n",
    "\\[\n",
    "F = \\frac{\\text{MSB}}{\\text{MSW}}\n",
    "\\]\n",
    "Where:  \n",
    "- **MSB** = $\\frac{SSB}{k - 1}$  \n",
    "- **MSW** = $\\frac{SSW}{N - k}$  \n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Calculate MSB**  \n",
    "Assuming:  \n",
    "- $k = 3$ (number of groups: Diet A, Diet B, Diet C)  \n",
    "- $N = 12$ (total number of observations)  \n",
    "\n",
    "\\[\n",
    "MSB = \\frac{SSB}{k - 1} = \\frac{6.14}{3 - 1} = \\frac{6.14}{2} = 3.07\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Calculate MSW**  \n",
    "Suppose we computed the total within-group sum of squares (SSW) to be 10.6.  \n",
    "\n",
    "\\[\n",
    "MSW = \\frac{SSW}{N - k} = \\frac{10.6}{12 - 3} = \\frac{10.6}{9} = 1.178\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Calculate the F-statistic**  \n",
    "\\[\n",
    "F = \\frac{\\text{MSB}}{\\text{MSW}} = \\frac{3.07}{1.178} = 2.606\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## **4️⃣ Check the p-value**\n",
    "To check if this F-statistic is significant, we compare it to a **critical F-value** from the **F-distribution table** for:  \n",
    "- Degrees of Freedom for numerator ($df_1 = k - 1 = 2$)  \n",
    "- Degrees of Freedom for denominator ($df_2 = N - k = 9$)  \n",
    "- Significance level ($\\alpha = 0.05$)  \n",
    "\n",
    "From the F-table, the critical F-value for $df_1 = 2$ and $df_2 = 9$ at **$\\alpha = 0.05$** is approximately **4.26**.\n",
    "\n",
    "---\n",
    "\n",
    "## **5️⃣ Decision**\n",
    "- Our calculated **F = 2.606**.  \n",
    "- The critical F-value from the table is **4.26**.  \n",
    "\n",
    "Since **F = 2.606** is **less than 4.26**, we **fail to reject the null hypothesis**.  \n",
    "This means there is **not enough evidence** to say that the group means are significantly different.\n",
    "\n",
    "---\n",
    "\n",
    "## **6️⃣ Conclusion**\n",
    "- **Null Hypothesis ($H_0$)**: The means of the 3 diets are equal.  \n",
    "- **Decision**: We **fail to reject the null hypothesis** because the F-statistic (2.606) is less than the critical F-value (4.26).  \n",
    "- **Conclusion**: There is no significant difference between the means of the 3 diets at the **0.05 significance level**.  \n",
    "\n",
    "---\n",
    "\n",
    "If you'd like to see this example calculated step-by-step in Python, I can provide the code as well. Let me know! 😊\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d60cbabb-4559-4502-8474-85a41fcbb803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from scipy import stats \n",
    "fertilizers = pd.read_csv(\"Data/fetilizers.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abc9caa2-94aa-43d3-934a-36c2bb97358d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fertilizer1</th>\n",
       "      <th>fertilizer2</th>\n",
       "      <th>fertilizer3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62</td>\n",
       "      <td>54</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>56</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90</td>\n",
       "      <td>58</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>36</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84</td>\n",
       "      <td>72</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>34</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fertilizer1  fertilizer2  fertilizer3\n",
       "0           62           54           48\n",
       "1           62           56           62\n",
       "2           90           58           92\n",
       "3           42           36           96\n",
       "4           84           72           92\n",
       "5           64           34           80"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fertilizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1bda36e-2796-475f-910e-20c029075d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistic : 3.66 , p-value : 0.051\n"
     ]
    }
   ],
   "source": [
    "one_way_anova = stats.f_oneway(fertilizers[\"fertilizer1\"], fertilizers[\"fertilizer2\"], fertilizers[\"fertilizer3\"]) \n",
    "print (\"Statistic :\", round(one_way_anova[0],2),\", p-value :\",round(one_way_anova[1],3)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311875bf-4705-40f6-b928-76f99beedbf9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Classification Metrics Example**\n",
    "\n",
    "### **Scenario**\n",
    "We are developing a machine learning model to predict if a patient has a disease. There are **50 patients** tested, and for each patient, we have the **actual label** (Does the patient have the disease? Yes or No) and the **model's prediction**.\n",
    "\n",
    "Here is a breakdown of the results:\n",
    "\n",
    "| **Patient** | **Actual** | **Predicted** |\n",
    "|-------------|------------|---------------|\n",
    "| 1           | Yes        | Yes           |\n",
    "| 2           | No         | No            |\n",
    "| 3           | Yes        | No            |\n",
    "| 4           | Yes        | Yes           |\n",
    "| 5           | No         | No            |\n",
    "| 6           | No         | Yes           |\n",
    "| 7           | Yes        | Yes           |\n",
    "| 8           | No         | No            |\n",
    "| 9           | Yes        | No            |\n",
    "| 10          | No         | No            |\n",
    "| 11          | Yes        | Yes           |\n",
    "| 12          | Yes        | Yes           |\n",
    "| 13          | No         | Yes           |\n",
    "| 14          | No         | No            |\n",
    "| 15          | Yes        | No            |\n",
    "| 16          | Yes        | Yes           |\n",
    "| 17          | No         | No            |\n",
    "| 18          | No         | No            |\n",
    "| 19          | Yes        | Yes           |\n",
    "| 20          | No         | Yes           |\n",
    "| 21          | Yes        | No            |\n",
    "| 22          | Yes        | Yes           |\n",
    "| 23          | No         | No            |\n",
    "| 24          | No         | No            |\n",
    "| 25          | Yes        | No            |\n",
    "| 26          | No         | Yes           |\n",
    "| 27          | No         | No            |\n",
    "| 28          | Yes        | Yes           |\n",
    "| 29          | Yes        | No            |\n",
    "| 30          | No         | No            |\n",
    "| 31          | No         | Yes           |\n",
    "| 32          | No         | Yes           |\n",
    "| 33          | Yes        | Yes           |\n",
    "| 34          | Yes        | No            |\n",
    "| 35          | No         | No            |\n",
    "| 36          | Yes        | Yes           |\n",
    "| 37          | No         | No            |\n",
    "| 38          | Yes        | Yes           |\n",
    "| 39          | No         | No            |\n",
    "| 40          | No         | Yes           |\n",
    "| 41          | Yes        | Yes           |\n",
    "| 42          | Yes        | Yes           |\n",
    "| 43          | No         | No            |\n",
    "| 44          | No         | No            |\n",
    "| 45          | Yes        | No            |\n",
    "| 46          | No         | Yes           |\n",
    "| 47          | No         | No            |\n",
    "| 48          | No         | Yes           |\n",
    "| 49          | Yes        | Yes           |\n",
    "| 50          | No         | No            |\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 1: Calculate TP, TN, FP, FN**\n",
    "- **True Positives (TP)**: 15\n",
    "- **True Negatives (TN)**: 18\n",
    "- **False Positives (FP)**: 9\n",
    "- **False Negatives (FN)**: 8\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 2: Calculate Evaluation Metrics**\n",
    "\n",
    "### **1. Precision (P)**\n",
    "$ \\text{Precision} = \\frac{TP}{TP + FP} = \\frac{15}{15 + 9} = \\frac{15}{24} = 0.625 $\n",
    "**Interpretation**: When the model predicted \"Yes\", it was correct 62.5% of the time.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Recall (R) / Sensitivity / True Positive Rate (TPR)**\n",
    "$ \\text{Recall} = \\frac{TP}{TP + FN} = \\frac{15}{15 + 8} = \\frac{15}{23} \\approx 0.652 $\n",
    "**Interpretation**: Out of all the actual \"Yes\" cases, the model correctly identified 65.2% of them.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. F1-Score (F1)**\n",
    "$ F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} $\n",
    "$ F1 = 2 \\cdot \\frac{0.625 \\cdot 0.652}{0.625 + 0.652} = 2 \\cdot \\frac{0.407}{1.277} = 0.637 $\n",
    "**Interpretation**: The F1 score is 0.637, indicating a balance between precision and recall.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Specificity (True Negative Rate)**\n",
    "$ \\text{Specificity} = \\frac{TN}{TN + FP} = \\frac{18}{18 + 9} = \\frac{18}{27} = 0.666 $\n",
    "**Interpretation**: Out of all the \"No\" cases, the model correctly identified 66.6% of them.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. False Positive Rate (FPR)**\n",
    "$ \\text{False Positive Rate} = \\frac{FP}{FP + TN} = \\frac{9}{9 + 18} = \\frac{9}{27} = 0.333 $\n",
    "**Note**: $ \\text{Specificity} = 1 - \\text{False Positive Rate} = 1 - 0.333 = 0.666 $\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Area Under ROC Curve (AUC)**\n",
    "The approximate AUC score for this model is **0.7**, as the TPR (0.652) is significantly higher than the FPR (0.333).\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary of Metrics**\n",
    "| **Metric**       | **Formula**                | **Value**   |\n",
    "|------------------|---------------------------|-------------|\n",
    "| **True Positives (TP)** | —                     | 15          |\n",
    "| **True Negatives (TN)** | —                     | 18          |\n",
    "| **False Positives (FP)** | —                    | 9           |\n",
    "| **False Negatives (FN)** | —                    | 8           |\n",
    "| **Precision**    | $ \\frac{TP}{TP + FP} $  | 0.625       |\n",
    "| **Recall (TPR)** | $ \\frac{TP}{TP + FN} $  | 0.652       |\n",
    "| **F1 Score**     | $ 2 \\cdot \\frac{P \\cdot R}{P + R} $ | 0.637  |\n",
    "| **Specificity**  | $ \\frac{TN}{TN + FP} $  | 0.666       |\n",
    "| **False Positive Rate (FPR)** | $ \\frac{FP}{FP + TN} $ | 0.333 |\n",
    "| **AUC**          | —                         | 0.7         |\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Takeaways**\n",
    "- **Precision**: When the model says \"Yes\", it is correct **62.5%** of the time.\n",
    "- **Recall**: Out of all people with the disease, the model identifies **65.2%**.\n",
    "- **Specificity**: Out of all people without the disease, the model identifies **66.6%**.\n",
    "- **F1-Score**: Balances precision and recall to **63.7%**.\n",
    "- **AUC**: The model is **70%** effective at distinguishing \"Yes\" from \"No\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fab9aa2-8b35-4262-9482-a7b8a5bd212d",
   "metadata": {},
   "source": [
    "## Adjusted R-squared value is the key metric in evaluating the quality of linear regressions. Any linear regression model having the value of R2 adjusted >= 0.7 is considered as a good enough model to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b104e2-1fb7-4ff1-8196-a131f084a849",
   "metadata": {},
   "source": [
    "# Bias vs. Variance Trade-off\n",
    "\n",
    "---\n",
    "\n",
    "## **What is Bias?**\n",
    "- **Bias** refers to errors caused by overly simplistic assumptions in the model.\n",
    "- A high-bias model fails to capture the complexity of the data, leading to **underfitting**.\n",
    "- Underfitting happens when the model is too simple to explain the data well, causing both the training and testing errors to be high.\n",
    "\n",
    "### **Example of High-Bias Models**:\n",
    "- **Linear Regression**:\n",
    "  - Imagine your data follows a complex curve, but linear regression tries to fit a straight line.\n",
    "  - The model won’t capture the actual relationships well, leaving high errors.\n",
    "\n",
    "---\n",
    "\n",
    "## **What is Variance?**\n",
    "- **Variance** refers to how sensitive a model is to fluctuations in the training data.\n",
    "- A high-variance model captures even the noise in the training data, leading to **overfitting**.\n",
    "- Overfitting happens when the model is too complex, performing well on training data but poorly on new data.\n",
    "\n",
    "### **Example of High-Variance Models**:\n",
    "- **Decision Trees**:\n",
    "  - Decision trees can create very detailed, complex models.\n",
    "  - Even small changes in the training data can result in drastically different trees, leading to instability.\n",
    "\n",
    "---\n",
    "\n",
    "## **Bias-Variance Trade-off**\n",
    "- Bias and variance are like a balancing act:\n",
    "  - **High bias** → The model is too simple → Underfitting.\n",
    "  - **High variance** → The model is too complex → Overfitting.\n",
    "- **Goal**: Find the right balance between bias and variance to minimize the total error.\n",
    "\n",
    "### **Total Error = Bias² + Variance + Irreducible Error**\n",
    "- **Irreducible error** is noise in the data that no model can fix.\n",
    "\n",
    "---\n",
    "\n",
    "## **Visualizing the Trade-off**\n",
    "1. **High Bias**:\n",
    "   - Predictions are far from the actual values.\n",
    "   - The model doesn’t capture the data’s complexity.\n",
    "   - Example: A straight line when the data forms a curve.\n",
    "\n",
    "2. **High Variance**:\n",
    "   - The model fits the training data almost perfectly.\n",
    "   - Poor generalization to new data (test data error is high).\n",
    "   - Example: A squiggly curve that tries to follow every point in the training set.\n",
    "\n",
    "---\n",
    "\n",
    "## **Modern Solution: Ensemble Methods**\n",
    "- Modern machine learning techniques balance bias and variance effectively using **ensemble methods**.\n",
    "- **Random Forest** is a great example:\n",
    "  - It combines multiple high-variance models (decision trees).\n",
    "  - By averaging their outputs, the overall variance decreases while keeping bias low.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Takeaways**\n",
    "- **High Bias = Underfitting**: The model is too simple to capture patterns in the data.\n",
    "- **High Variance = Overfitting**: The model is too complex, capturing noise instead of patterns.\n",
    "- **Balanced Model**: The sweet spot has both low bias and low variance, achieving good generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beab1d2-e3a0-40a0-a115-7adbd21a069f",
   "metadata": {},
   "source": [
    "# Statistical Modeling vs Machine Learning\n",
    "\n",
    "## Statistical Modeling\n",
    "- **Example**: Linear regression with two independent variables.\n",
    "- **Goal**: Fits the **best plane** through the data points by minimizing errors.\n",
    "- Focuses on understanding the **relationship** between the independent variables (inputs) and the dependent variable (output).\n",
    "\n",
    "## Machine Learning\n",
    "- **Focus**: Optimizes parameters (e.g., weights and biases) rather than just relationships between variables.\n",
    "- Converts the problem into an **optimization task** (minimizing a function like squared error).\n",
    "- Errors are **squared** to make the function **convex**, ensuring:\n",
    "  - Faster convergence.\n",
    "  - A global optimum is reached.\n",
    "\n",
    "---\n",
    "\n",
    "# Convex vs Non-Convex Functions\n",
    "\n",
    "## Convex Functions\n",
    "- A function is convex if a straight line drawn between any two points on the curve always stays **above or on the curve**.\n",
    "- In convex functions:\n",
    "  - **Local minimum** = **Global minimum**.\n",
    "- Optimization techniques like **gradient descent** work reliably and find the best solution.\n",
    "\n",
    "## Non-Convex Functions\n",
    "- A function is non-convex if a straight line between two points on the curve can sometimes go **below the curve**.\n",
    "- In non-convex functions:\n",
    "  - There can be **multiple local minima** (valleys).\n",
    "  - It's hard to know if the solution is the **global minimum** (best solution).\n",
    "\n",
    "---\n",
    "\n",
    "# Why Does This Matter in Machine Learning?\n",
    "- Machine learning models rely on **optimization** to find the best parameters (e.g., weights, biases).\n",
    "- If the function being optimized is convex:\n",
    "  - The optimization process (e.g., **gradient descent**) is **guaranteed to find the global minimum**.\n",
    "- If the function is non-convex:\n",
    "  - The optimization process might get \"stuck\" in a local minimum and fail to find the best solution.\n",
    "\n",
    "---\n",
    "\n",
    "# Key Takeaways\n",
    "1. Machine learning converts the problem into an **optimization task**.\n",
    "2. Squaring the errors creates a **convex function**, which ensures:\n",
    "   - Faster convergence.\n",
    "   - The global optimum is achieved.\n",
    "3. For non-convex problems:\n",
    "   - Advanced techniques (e.g., adding randomness or ensemble methods) are used to overcome the challenge of multiple minima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c214556-5654-47be-8812-11f0fa6d5682",
   "metadata": {},
   "source": [
    "# Convex and Non-Convex Functions\n",
    "\n",
    "### **Convex Function**\n",
    "\n",
    "A function is **convex** if the line segment connecting any two points on the function lies **above or on the curve**.\n",
    "\n",
    "#### **Example 1: Quadratic Function**\n",
    "The function:  \n",
    "$$f(x) = x^2$$  \n",
    "This is a classic example of a convex function. If you take any two points, say $x_1 = -1$ and $x_2 = 2$, the line connecting $f(-1)$ and $f(2)$ will always lie **above or on** the curve of $f(x)$.\n",
    "\n",
    "#### **Key Characteristics:**\n",
    "1. Has a single **global minimum** (no other \"valleys\").\n",
    "2. Easy to optimize (find the minimum).\n",
    "\n",
    "#### **Real-Life Analogy**:\n",
    "Imagine you're at the bottom of a perfectly smooth **bowl**. No matter where you start on the edge of the bowl, you'll always slide down to the **lowest point**, which is the **global minimum**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Non-Convex Function**\n",
    "\n",
    "A function is **non-convex** if the line segment connecting two points on the function can lie **below the curve**.\n",
    "\n",
    "#### **Example 2: Sinusoidal Function**\n",
    "The function:  \n",
    "$$f(x) = \\sin(x)$$  \n",
    "This is an example of a non-convex function. It has multiple **peaks** and **valleys** (local minima and maxima). If you take two points on this curve, say $x_1 = \\pi/4$ and $x_2 = 3\\pi/4$, the line connecting $f(\\pi/4)$ and $f(3\\pi/4)$ will fall **below the curve**.\n",
    "\n",
    "#### **Key Characteristics:**\n",
    "1. Has **multiple local minima** (valleys) and maxima (peaks).\n",
    "2. Hard to optimize because an algorithm may \"get stuck\" in a **local minimum** and fail to find the **global minimum**.\n",
    "\n",
    "#### **Real-Life Analogy**:\n",
    "Imagine you're hiking in a mountainous region with multiple peaks and valleys. If you're trying to find the lowest point, you might get stuck in one of the small valleys (local minimum) and fail to reach the deepest valley (global minimum).\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical Implications in Machine Learning**\n",
    "\n",
    "1. **Convex Functions**:\n",
    "   - Optimization (like gradient descent) is straightforward.\n",
    "   - Example: Linear regression minimizes the squared error function, which is convex.\n",
    "\n",
    "2. **Non-Convex Functions**:\n",
    "   - Optimization is harder because algorithms might get stuck in local minima.\n",
    "   - Example: Neural networks often involve non-convex functions due to the complexity of the loss function. Advanced techniques like random initialization, momentum, or Adam optimizer are used to overcome this challenge.\n",
    "\n",
    "---\n",
    "\n",
    "### **Visualization**\n",
    "\n",
    "1. **Convex Function**: A smooth \"U\" shape, like a bowl.  \n",
    "   Example: $f(x) = x^2$\n",
    "\n",
    "2. **Non-Convex Function**: A wavy curve with multiple peaks and valleys.  \n",
    "   Example: $f(x) = \\sin(x)$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37f2f84-9174-4938-b048-cc361ca1c7dd",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9679df18-2941-4d61-a4cf-352c2a956ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged, iterations: 543935\n",
      "Gradient Descent Results:\n",
      "Intercept = 4.225953652228333, Coefficient = 2.6128610303074944\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(x, y, learn_rate, conv_threshold, batch_size, max_iter):\n",
    "    converged = False\n",
    "    iter = 0\n",
    "    m = batch_size\n",
    "    t0 = np.random.random()  # Initialize intercept randomly\n",
    "    t1 = np.random.random()  # Initialize coefficient randomly\n",
    "    MSE = float('inf')  # Initialize MSE to a large value\n",
    "\n",
    "    while not converged:\n",
    "        grad0 = 1.0 / m * sum([(t0 + t1 * x[i] - y[i]) for i in range(m)])\n",
    "        grad1 = 1.0 / m * sum([(t0 + t1 * x[i] - y[i]) * x[i] for i in range(m)])\n",
    "        \n",
    "        temp0 = t0 - learn_rate * grad0\n",
    "        temp1 = t1 - learn_rate * grad1\n",
    "\n",
    "        t0, t1 = temp0, temp1\n",
    "\n",
    "        MSE_New = sum([(t0 + t1 * x[i] - y[i]) ** 2 for i in range(m)]) / m\n",
    "\n",
    "        if abs(MSE - MSE_New) <= conv_threshold:\n",
    "            print('Converged, iterations:', iter)\n",
    "            converged = True\n",
    "        \n",
    "        MSE = MSE_New\n",
    "        iter += 1\n",
    "\n",
    "        if iter == max_iter:\n",
    "            print('Max iterations reached')\n",
    "            converged = True\n",
    "\n",
    "    return t0, t1\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "# Generate example data\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1).flatten()\n",
    "y = 4 + 3 * X + np.random.randn(100)\n",
    "\n",
    "# Perform gradient descent\n",
    "Inter, Coeff = gradient_descent(\n",
    "    x=X,\n",
    "    y=y,\n",
    "    learn_rate=0.00003,\n",
    "    conv_threshold=1e-8,\n",
    "    batch_size=32,\n",
    "    max_iter=1500000\n",
    ")\n",
    "print('Gradient Descent Results:')\n",
    "print(f'Intercept = {Inter}, Coefficient = {Coeff}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aec371-7df6-4762-9526-0d93c97f8c5a",
   "metadata": {},
   "source": [
    "# **Gradient Descent Explained with Code and Example**\n",
    "\n",
    "---\n",
    "\n",
    "## **Mathematical Formulation**\n",
    "\n",
    "Gradient Descent minimizes the Mean Squared Error (MSE), the objective function for linear regression.\n",
    "\n",
    "1. **Objective Function:**\n",
    "   $$\n",
    "   J(\\theta_0, \\theta_1) = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x_i) - y_i \\right)^2\n",
    "   $$\n",
    "   - $J(\\theta_0, \\theta_1)$: MSE\n",
    "   - $m$: Number of samples in the batch\n",
    "   - $h_\\theta(x_i) = \\theta_0 + \\theta_1 x_i$: Predicted value\n",
    "   - $y_i$: Actual value\n",
    "\n",
    "2. **Gradients:**\n",
    "   $$\n",
    "   \\frac{\\partial J}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x_i) - y_i \\right)\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial J}{\\partial \\theta_1} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x_i) - y_i \\right) x_i\n",
    "   $$\n",
    "\n",
    "3. **Parameter Updates:**\n",
    "   $$\n",
    "   \\theta_0 \\leftarrow \\theta_0 - \\alpha \\frac{\\partial J}{\\partial \\theta_0}\n",
    "   $$\n",
    "   $$\n",
    "   \\theta_1 \\leftarrow \\theta_1 - \\alpha \\frac{\\partial J}{\\partial \\theta_1}\n",
    "   $$\n",
    "   - $\\alpha$: Learning rate\n",
    "\n",
    "4. **Convergence Condition:**\n",
    "   $$\n",
    "   | J_{\\text{old}} - J_{\\text{new}} | \\leq \\text{conv\\_threshold}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## **Worked Example**\n",
    "\n",
    "### **Dataset:**\n",
    "- $x = [1, 2, 3, 4]$ (input values)\n",
    "- $y = [2.2, 2.8, 4.5, 3.7]$ (actual values)\n",
    "\n",
    "### **Initial Parameters:**\n",
    "- $\\theta_0 = 0.5, \\theta_1 = 0.5$\n",
    "- Learning rate $\\alpha = 0.01$\n",
    "\n",
    "### **Iterations:**\n",
    "\n",
    "- Compute predictions $h_\\theta(x) = \\theta_0 + \\theta_1 x$\n",
    "- Compute gradients:\n",
    "  $$\n",
    "  \\text{grad}_0 = \\frac{1}{m} \\sum \\left( h_\\theta(x_i) - y_i \\right)\n",
    "  $$\n",
    "  $$\n",
    "  \\text{grad}_1 = \\frac{1}{m} \\sum \\left( h_\\theta(x_i) - y_i \\right) x_i\n",
    "  $$\n",
    "- Update parameters:\n",
    "  $$\n",
    "  \\theta_0 \\leftarrow \\theta_0 - \\alpha \\cdot \\text{grad}_0\n",
    "  $$\n",
    "  $$\n",
    "  \\theta_1 \\leftarrow \\theta_1 - \\alpha \\cdot \\text{grad}_1\n",
    "  $$\n",
    "\n",
    "- Repeat Until Convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bb47d3-266d-475b-8958-ad60167df592",
   "metadata": {},
   "source": [
    "# **Step-by-Step Calculation for Gradient Descent Iteration**\n",
    "\n",
    "### **Dataset:**\n",
    "- $x = [1, 2, 3, 4]$ (input values)\n",
    "- $y = [2.2, 2.8, 4.5, 3.7]$ (actual values)\n",
    "\n",
    "### **Initial Parameters:**\n",
    "- $\\theta_0 = 0.5, \\theta_1 = 0.5$\n",
    "- Learning rate $\\alpha = 0.01$\n",
    "- Number of samples $m = 4$ (since there are 4 data points)\n",
    "\n",
    "### **Iteration 1:**\n",
    "\n",
    "1. **Compute Predictions:**\n",
    "\n",
    "   For each value of $x_i$, the prediction is:\n",
    "   $$\n",
    "   h_\\theta(x_i) = \\theta_0 + \\theta_1 x_i\n",
    "   $$\n",
    "\n",
    "   Using the initial values of $\\theta_0 = 0.5$ and $\\theta_1 = 0.5$, we compute the predictions for each $x_i$:\n",
    "   - For $x_1 = 1$, $h_\\theta(x_1) = 0.5 + 0.5(1) = 1.0$\n",
    "   - For $x_2 = 2$, $h_\\theta(x_2) = 0.5 + 0.5(2) = 1.5$\n",
    "   - For $x_3 = 3$, $h_\\theta(x_3) = 0.5 + 0.5(3) = 2.0$\n",
    "   - For $x_4 = 4$, $h_\\theta(x_4) = 0.5 + 0.5(4) = 2.5$\n",
    "\n",
    "   Thus, the predictions are:\n",
    "   $$\n",
    "   h_\\theta = [1.0, 1.5, 2.0, 2.5]\n",
    "   $$\n",
    "\n",
    "2. **Compute Gradients:**\n",
    "\n",
    "   The gradients for $\\theta_0$ and $\\theta_1$ are calculated as:\n",
    "   $$\n",
    "   \\text{grad}_0 = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x_i) - y_i \\right)\n",
    "   $$\n",
    "   $$\n",
    "   \\text{grad}_1 = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x_i) - y_i \\right) x_i\n",
    "   $$\n",
    "\n",
    "   We compute the individual errors $(h_\\theta(x_i) - y_i)$ for each $i$:\n",
    "   - For $x_1 = 1$, error = $1.0 - 2.2 = -1.2$\n",
    "   - For $x_2 = 2$, error = $1.5 - 2.8 = -1.3$\n",
    "   - For $x_3 = 3$, error = $2.0 - 4.5 = -2.5$\n",
    "   - For $x_4 = 4$, error = $2.5 - 3.7 = -1.2$\n",
    "\n",
    "   Now, calculate the gradients:\n",
    "   - $\\text{grad}_0$:\n",
    "     $$\n",
    "     \\text{grad}_0 = \\frac{1}{4} \\left( (-1.2) + (-1.3) + (-2.5) + (-1.2) \\right) = \\frac{1}{4} \\times (-6.2) = -1.55\n",
    "     $$\n",
    "   - $\\text{grad}_1$:\n",
    "     $$\n",
    "     \\text{grad}_1 = \\frac{1}{4} \\left( (-1.2)(1) + (-1.3)(2) + (-2.5)(3) + (-1.2)(4) \\right)\n",
    "     $$\n",
    "     $$\n",
    "     \\text{grad}_1 = \\frac{1}{4} \\left( -1.2 - 2.6 - 7.5 - 4.8 \\right) = \\frac{1}{4} \\times (-16.1) = -4.025\n",
    "     $$\n",
    "\n",
    "3. **Update Parameters:**\n",
    "\n",
    "   Now, update the parameters using the gradients and learning rate:\n",
    "   $$\n",
    "   \\theta_0 \\leftarrow \\theta_0 - \\alpha \\cdot \\text{grad}_0\n",
    "   $$\n",
    "   $$\n",
    "   \\theta_1 \\leftarrow \\theta_1 - \\alpha \\cdot \\text{grad}_1\n",
    "   $$\n",
    "\n",
    "   Using $\\alpha = 0.01$, we compute the new values for $\\theta_0$ and $\\theta_1$:\n",
    "   - Update $\\theta_0$:\n",
    "     $$\n",
    "     \\theta_0 = 0.5 - 0.01 \\cdot (-1.55) = 0.5 + 0.0155 = 0.5155\n",
    "     $$\n",
    "   - Update $\\theta_1$:\n",
    "     $$\n",
    "     \\theta_1 = 0.5 - 0.01 \\cdot (-4.025) = 0.5 + 0.04025 = 0.54025\n",
    "     $$\n",
    "\n",
    "### **New Parameters After Iteration 1:**\n",
    "- $\\theta_0 = 0.5155$\n",
    "- $\\theta_1 = 0.54025$\n",
    "\n",
    "---\n",
    "\n",
    "### **Iteration 2:**\n",
    "\n",
    "1. **Compute New Predictions:**\n",
    "   \n",
    "   Using the updated values of $\\theta_0 = 0.5155$ and $\\theta_1 = 0.54025$:\n",
    "   - For $x_1 = 1$, $h_\\theta(x_1) = 0.5155 + 0.54025(1) = 1.05575$\n",
    "   - For $x_2 = 2$, $h_\\theta(x_2) = 0.5155 + 0.54025(2) = 1.59625$\n",
    "   - For $x_3 = 3$, $h_\\theta(x_3) = 0.5155 + 0.54025(3) = 2.13675$\n",
    "   - For $x_4 = 4$, $h_\\theta(x_4) = 0.5155 + 0.54025(4) = 2.67725$\n",
    "\n",
    "2. **Compute Gradients:**\n",
    "\n",
    "   Calculate the errors and gradients:\n",
    "   - For $x_1 = 1$, error = $1.05575 - 2.2 = -1.14425$\n",
    "   - For $x_2 = 2$, error = $1.59625 - 2.8 = -1.20375$\n",
    "   - For $x_3 = 3$, error = $2.13675 - 4.5 = -2.36325$\n",
    "   - For $x_4 = 4$, error = $2.67725 - 3.7 = -1.02275$\n",
    "\n",
    "   Then compute the gradients:\n",
    "   - $\\text{grad}_0$:\n",
    "     $$\n",
    "     \\text{grad}_0 = \\frac{1}{4} \\left( (-1.14425) + (-1.20375) + (-2.36325) + (-1.02275) \\right) = \\frac{1}{4} \\times (-5.734) = -1.4335\n",
    "     $$\n",
    "   - $\\text{grad}_1$:\n",
    "     $$\n",
    "     \\text{grad}_1 = \\frac{1}{4} \\left( (-1.14425)(1) + (-1.20375)(2) + (-2.36325)(3) + (-1.02275)(4) \\right)\n",
    "     $$\n",
    "     $$\n",
    "     \\text{grad}_1 = \\frac{1}{4} \\left( -1.14425 - 2.4075 - 7.08975 - 4.091 \\right) = \\frac{1}{4} \\times (-14.7325) = -3.683125\n",
    "     $$\n",
    "\n",
    "3. **Update Parameters:**\n",
    "\n",
    "   - Update $\\theta_0$:\n",
    "     $$\n",
    "     \\theta_0 = 0.5155 - 0.01 \\cdot (-1.4335) = 0.5155 + 0.014335 = 0.529835\n",
    "     $$\n",
    "\n",
    "   - Update $\\theta_1$:\n",
    "     $$\n",
    "     \\theta_1 = 0.54025 - 0.01 \\cdot (-3.683125) = 0.54025 + 0.03683125 = 0.57708125\n",
    "     $$\n",
    "\n",
    "### **New Parameters After Iteration 2:**\n",
    "- $\\theta_0 = 0.529835$\n",
    "- $\\theta_1 = 0.57708125$\n",
    "\n",
    "---\n",
    "\n",
    "### **Continue the Process:**\n",
    "Repeat the above steps (compute predictions, gradients, and parameter updates) until the parameters converge, i.e., the change in MSE between iterations is less than a predefined convergence threshold (e.g., $10^{-8}$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5015db06-2a11-48af-9c86-ace8ac39edb3",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
